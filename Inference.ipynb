{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c36f2d6-9c4e-4eed-bc6e-72e2682c8ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Thu_Nov_18_09:45:30_PST_2021\n",
      "Cuda compilation tools, release 11.5, V11.5.119\n",
      "Build cuda_11.5.r11.5/compiler.30672275_0\n",
      "torch:  2.1 ; cuda:  cu121\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41224869-a0df-4568-9619-57450b789f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from time import time\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d18a67e-c54b-495d-a83b-da6774db7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using the pre-trained Detectron2 model, as shown below.\n",
    "cfg = get_cfg()\n",
    "\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "# load the pre trained model from Detectron2 model zoo\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\"))\n",
    "# set confidence threshold for this model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "# load model weights\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\")\n",
    "# create the predictor for pose estimation using the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e3ef37-1942-48b4-adc7-9a71deeb80d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/17 16:53:48 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x/138363331/model_final_997cc7.pkl ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b480a557-4ef1-4fd1-b34a-72be13db0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco2h36m(x):\n",
    "    '''\n",
    "        Input: x (M x T x V x C)\n",
    "\n",
    "        COCO: {0-nose 1-Leye 2-Reye 3-Lear 4Rear 5-Lsho 6-Rsho 7-Lelb 8-Relb 9-Lwri 10-Rwri 11-Lhip 12-Rhip 13-Lkne 14-Rkne 15-Lank 16-Rank}\n",
    "\n",
    "        H36M:\n",
    "        0: 'root',\n",
    "        1: 'rhip',\n",
    "        2: 'rkne',\n",
    "        3: 'rank',\n",
    "        4: 'lhip',\n",
    "        5: 'lkne',\n",
    "        6: 'lank',\n",
    "        7: 'belly',\n",
    "        8: 'neck',\n",
    "        9: 'nose',\n",
    "        10: 'head',\n",
    "        11: 'lsho',\n",
    "        12: 'lelb',\n",
    "        13: 'lwri',\n",
    "        14: 'rsho',\n",
    "        15: 'relb',\n",
    "        16: 'rwri'\n",
    "    '''\n",
    "    y = np.zeros(x.shape)\n",
    "    y[:,0,:] = (x[:,11,:] + x[:,12,:]) * 0.5\n",
    "    y[:,1,:] = x[:,12,:]\n",
    "    y[:,2,:] = x[:,14,:]\n",
    "    y[:,3,:] = x[:,16,:]\n",
    "    y[:,4,:] = x[:,11,:]\n",
    "    y[:,5,:] = x[:,13,:]\n",
    "    y[:,6,:] = x[:,15,:]\n",
    "    y[:,8,:] = (x[:,5,:] + x[:,6,:]) * 0.5\n",
    "    y[:,7,:] = (y[:,0,:] + y[:,8,:]) * 0.5\n",
    "    y[:,9,:] = x[:,0,:]\n",
    "    y[:,10,:] = (x[:,1,:] + x[:,2,:]) * 0.5\n",
    "    y[:,11,:] = x[:,5,:]\n",
    "    y[:,12,:] = x[:,7,:]\n",
    "    y[:,13,:] = x[:,9,:]\n",
    "    y[:,14,:] = x[:,6,:]\n",
    "    y[:,15,:] = x[:,8,:]\n",
    "    y[:,16,:] = x[:,10,:]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9003b119-4fdd-40e4-9697-f9a60a9b9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import clip\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0274f6-f516-41e2-ae0e-6c15ebec0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "# from common.skeleton import Skeleton\n",
    "# from common.mocap_dataset import MocapDataset\n",
    "# from common.camera import normalize_screen_coordinates, image_coordinates\n",
    "\n",
    "def normalize_screen_coordinates(X, w, h): \n",
    "    assert X.shape[-1] == 2\n",
    "    \n",
    "    # Normalize so that [0, w] is mapped to [-1, 1], while preserving the aspect ratio\n",
    "    return X/w*2 - [1, h/w]\n",
    "\n",
    "    \n",
    "def image_coordinates(X, w, h):\n",
    "    assert X.shape[-1] == 2\n",
    "    \n",
    "    # Reverse camera frame normalization\n",
    "    return (X + [1, h/w])*w/2\n",
    "    \n",
    "class Skeleton:\n",
    "    def __init__(self, parents, joints_left, joints_right):\n",
    "        assert len(joints_left) == len(joints_right)\n",
    "        \n",
    "        self._parents = np.array(parents)\n",
    "        self._joints_left = joints_left\n",
    "        self._joints_right = joints_right\n",
    "        self._compute_metadata()\n",
    "    \n",
    "    def num_joints(self):\n",
    "        return len(self._parents)\n",
    "    \n",
    "    def parents(self):\n",
    "        return self._parents\n",
    "    \n",
    "    def has_children(self):\n",
    "        return self._has_children\n",
    "    \n",
    "    def children(self):\n",
    "        return self._children\n",
    "    \n",
    "    def remove_joints(self, joints_to_remove):\n",
    "        \"\"\"\n",
    "        Remove the joints specified in 'joints_to_remove'.\n",
    "        \"\"\"\n",
    "        valid_joints = []\n",
    "        for joint in range(len(self._parents)):\n",
    "            if joint not in joints_to_remove:\n",
    "                valid_joints.append(joint)\n",
    "\n",
    "        for i in range(len(self._parents)):\n",
    "            while self._parents[i] in joints_to_remove:\n",
    "                self._parents[i] = self._parents[self._parents[i]]\n",
    "                \n",
    "        index_offsets = np.zeros(len(self._parents), dtype=int)\n",
    "        new_parents = []\n",
    "        for i, parent in enumerate(self._parents):\n",
    "            if i not in joints_to_remove:\n",
    "                new_parents.append(parent - index_offsets[parent])\n",
    "            else:\n",
    "                index_offsets[i:] += 1\n",
    "        self._parents = np.array(new_parents)\n",
    "        \n",
    "        \n",
    "        if self._joints_left is not None:\n",
    "            new_joints_left = []\n",
    "            for joint in self._joints_left:\n",
    "                if joint in valid_joints:\n",
    "                    new_joints_left.append(joint - index_offsets[joint])\n",
    "            self._joints_left = new_joints_left\n",
    "        if self._joints_right is not None:\n",
    "            new_joints_right = []\n",
    "            for joint in self._joints_right:\n",
    "                if joint in valid_joints:\n",
    "                    new_joints_right.append(joint - index_offsets[joint])\n",
    "            self._joints_right = new_joints_right\n",
    "\n",
    "        self._compute_metadata()\n",
    "        \n",
    "        return valid_joints\n",
    "    \n",
    "    def joints_left(self):\n",
    "        return self._joints_left\n",
    "    \n",
    "    def joints_right(self):\n",
    "        return self._joints_right\n",
    "        \n",
    "    def _compute_metadata(self):\n",
    "        self._has_children = np.zeros(len(self._parents)).astype(bool)\n",
    "        for i, parent in enumerate(self._parents):\n",
    "            if parent != -1:\n",
    "                self._has_children[parent] = True\n",
    "\n",
    "        self._children = []\n",
    "        for i, parent in enumerate(self._parents):\n",
    "            self._children.append([])\n",
    "        for i, parent in enumerate(self._parents):\n",
    "            if parent != -1:\n",
    "                self._children[parent].append(i)\n",
    "\n",
    "class MocapDataset:\n",
    "    def __init__(self, fps, skeleton):\n",
    "        self._skeleton = skeleton\n",
    "        self._fps = fps\n",
    "        self._data = None # Must be filled by subclass\n",
    "        self._cameras = None # Must be filled by subclass\n",
    "    \n",
    "    def remove_joints(self, joints_to_remove):\n",
    "        kept_joints = self._skeleton.remove_joints(joints_to_remove)\n",
    "        for subject in self._data.keys():\n",
    "            for action in self._data[subject].keys():\n",
    "                s = self._data[subject][action]\n",
    "                if 'positions' in s:\n",
    "                    s['positions'] = s['positions'][:, kept_joints]\n",
    "                \n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return self._data[key]\n",
    "        \n",
    "    def subjects(self):\n",
    "        return self._data.keys()\n",
    "    \n",
    "    def fps(self):\n",
    "        return self._fps\n",
    "    \n",
    "    def skeleton(self):\n",
    "        return self._skeleton\n",
    "        \n",
    "    def cameras(self):\n",
    "        return self._cameras\n",
    "    \n",
    "    def supports_semi_supervised(self):\n",
    "        # This method can be overridden\n",
    "        return False\n",
    "        \n",
    "h36m_skeleton = Skeleton(parents=[-1,  0,  1,  2,  3,  4,  0,  6,  7,  8,  9,  0, 11, 12, 13, 14, 12,\n",
    "       16, 17, 18, 19, 20, 19, 22, 12, 24, 25, 26, 27, 28, 27, 30],\n",
    "       joints_left=[6, 7, 8, 9, 10, 16, 17, 18, 19, 20, 21, 22, 23],\n",
    "       joints_right=[1, 2, 3, 4, 5, 24, 25, 26, 27, 28, 29, 30, 31])\n",
    "\n",
    "h36m_cameras_intrinsic_params = [\n",
    "    {\n",
    "        'id': '54138969',\n",
    "        'center': [512.54150390625, 515.4514770507812],\n",
    "        'focal_length': [1145.0494384765625, 1143.7811279296875],\n",
    "        'radial_distortion': [-0.20709891617298126, 0.24777518212795258, -0.0030751503072679043],\n",
    "        'tangential_distortion': [-0.0009756988729350269, -0.00142447161488235],\n",
    "        'res_w': 1000,\n",
    "        'res_h': 1002,\n",
    "        'azimuth': 70, # Only used for visualization\n",
    "    },\n",
    "    {\n",
    "        'id': '55011271',\n",
    "        'center': [508.8486328125, 508.0649108886719],\n",
    "        'focal_length': [1149.6756591796875, 1147.5916748046875],\n",
    "        'radial_distortion': [-0.1942136287689209, 0.2404085397720337, 0.006819975562393665],\n",
    "        'tangential_distortion': [-0.0016190266469493508, -0.0027408944442868233],\n",
    "        'res_w': 1000,\n",
    "        'res_h': 1000,\n",
    "        'azimuth': -70, # Only used for visualization\n",
    "    },\n",
    "    {\n",
    "        'id': '58860488',\n",
    "        'center': [519.8158569335938, 501.40264892578125],\n",
    "        'focal_length': [1149.1407470703125, 1148.7989501953125],\n",
    "        'radial_distortion': [-0.2083381861448288, 0.25548800826072693, -0.0024604974314570427],\n",
    "        'tangential_distortion': [0.0014843869721516967, -0.0007599993259645998],\n",
    "        'res_w': 1000,\n",
    "        'res_h': 1000,\n",
    "        'azimuth': 110, # Only used for visualization\n",
    "    },\n",
    "    {\n",
    "        'id': '60457274',\n",
    "        'center': [514.9682006835938, 501.88201904296875],\n",
    "        'focal_length': [1145.5113525390625, 1144.77392578125],\n",
    "        'radial_distortion': [-0.198384091258049, 0.21832367777824402, -0.008947807364165783],\n",
    "        'tangential_distortion': [-0.0005872055771760643, -0.0018133620033040643],\n",
    "        'res_w': 1000,\n",
    "        'res_h': 1002,\n",
    "        'azimuth': -110, # Only used for visualization\n",
    "    },\n",
    "]\n",
    "\n",
    "h36m_cameras_extrinsic_params = {\n",
    "    'S1': [\n",
    "        {\n",
    "            'orientation': [0.1407056450843811, -0.1500701755285263, -0.755240797996521, 0.6223280429840088],\n",
    "            'translation': [1841.1070556640625, 4955.28466796875, 1563.4454345703125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6157187819480896, -0.764836311340332, -0.14833825826644897, 0.11794740706682205],\n",
    "            'translation': [1761.278564453125, -5078.0068359375, 1606.2650146484375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.14651472866535187, -0.14647851884365082, 0.7653023600578308, -0.6094175577163696],\n",
    "            'translation': [-1846.7777099609375, 5215.04638671875, 1491.972412109375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5834008455276489, -0.7853162288665771, 0.14548823237419128, -0.14749594032764435],\n",
    "            'translation': [-1794.7896728515625, -3722.698974609375, 1574.8927001953125],\n",
    "        },\n",
    "    ],\n",
    "    'S2': [\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "    ],\n",
    "    'S3': [\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "    ],\n",
    "    'S4': [\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "    ],\n",
    "    'S5': [\n",
    "        {\n",
    "            'orientation': [0.1467377245426178, -0.162370964884758, -0.7551892995834351, 0.6178938746452332],\n",
    "            'translation': [2097.3916015625, 4880.94482421875, 1605.732421875],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6159758567810059, -0.7626792192459106, -0.15728192031383514, 0.1189815029501915],\n",
    "            'translation': [2031.7008056640625, -5167.93310546875, 1612.923095703125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.14291371405124664, -0.12907841801643372, 0.7678384780883789, -0.6110143065452576],\n",
    "            'translation': [-1620.5948486328125, 5171.65869140625, 1496.43701171875],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5920479893684387, -0.7814217805862427, 0.1274748593568802, -0.15036417543888092],\n",
    "            'translation': [-1637.1737060546875, -3867.3173828125, 1547.033203125],\n",
    "        },\n",
    "    ],\n",
    "    'S6': [\n",
    "        {\n",
    "            'orientation': [0.1337897777557373, -0.15692396461963654, -0.7571090459823608, 0.6198879480361938],\n",
    "            'translation': [1935.4517822265625, 4950.24560546875, 1618.0838623046875],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6147197484970093, -0.7628812789916992, -0.16174767911434174, 0.11819244921207428],\n",
    "            'translation': [1969.803955078125, -5128.73876953125, 1632.77880859375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.1529948115348816, -0.13529130816459656, 0.7646096348762512, -0.6112781167030334],\n",
    "            'translation': [-1769.596435546875, 5185.361328125, 1476.993408203125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5916101336479187, -0.7804774045944214, 0.12832270562648773, -0.1561593860387802],\n",
    "            'translation': [-1721.668701171875, -3884.13134765625, 1540.4879150390625],\n",
    "        },\n",
    "    ],\n",
    "    'S7': [\n",
    "        {\n",
    "            'orientation': [0.1435241848230362, -0.1631336808204651, -0.7548328638076782, 0.6188824772834778],\n",
    "            'translation': [1974.512939453125, 4926.3544921875, 1597.8326416015625],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6141672730445862, -0.7638262510299683, -0.1596645563840866, 0.1177929937839508],\n",
    "            'translation': [1937.0584716796875, -5119.7900390625, 1631.5665283203125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.14550060033798218, -0.12874816358089447, 0.7660516500473022, -0.6127139329910278],\n",
    "            'translation': [-1741.8111572265625, 5208.24951171875, 1464.8245849609375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5912848114967346, -0.7821764349937439, 0.12445473670959473, -0.15196487307548523],\n",
    "            'translation': [-1734.7105712890625, -3832.42138671875, 1548.5830078125],\n",
    "        },\n",
    "    ],\n",
    "    'S8': [\n",
    "        {\n",
    "            'orientation': [0.14110587537288666, -0.15589867532253265, -0.7561917304992676, 0.619644045829773],\n",
    "            'translation': [2150.65185546875, 4896.1611328125, 1611.9046630859375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6169601678848267, -0.7647668123245239, -0.14846350252628326, 0.11158157885074615],\n",
    "            'translation': [2219.965576171875, -5148.453125, 1613.0440673828125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.1471444070339203, -0.13377119600772858, 0.7670128345489502, -0.6100369691848755],\n",
    "            'translation': [-1571.2215576171875, 5137.0185546875, 1498.1761474609375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5927824378013611, -0.7825870513916016, 0.12147816270589828, -0.14631995558738708],\n",
    "            'translation': [-1476.913330078125, -3896.7412109375, 1547.97216796875],\n",
    "        },\n",
    "    ],\n",
    "    'S9': [\n",
    "        {\n",
    "            'orientation': [0.15540587902069092, -0.15548215806484222, -0.7532095313072205, 0.6199594736099243],\n",
    "            'translation': [2044.45849609375, 4935.1171875, 1481.2275390625],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.618784487247467, -0.7634735107421875, -0.14132238924503326, 0.11933968216180801],\n",
    "            'translation': [1990.959716796875, -5123.810546875, 1568.8048095703125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.13357827067375183, -0.1367100477218628, 0.7689454555511475, -0.6100738644599915],\n",
    "            'translation': [-1670.9921875, 5211.98583984375, 1528.387939453125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5879399180412292, -0.7823407053947449, 0.1427614390850067, -0.14794869720935822],\n",
    "            'translation': [-1696.04345703125, -3827.099853515625, 1591.4127197265625],\n",
    "        },\n",
    "    ],\n",
    "    'S11': [\n",
    "        {\n",
    "            'orientation': [0.15232472121715546, -0.15442320704460144, -0.7547563314437866, 0.6191070079803467],\n",
    "            'translation': [2098.440185546875, 4926.5546875, 1500.278564453125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6189449429512024, -0.7600917220115662, -0.15300633013248444, 0.1255258321762085],\n",
    "            'translation': [2083.182373046875, -4912.1728515625, 1561.07861328125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.14943228662014008, -0.15650227665901184, 0.7681233882904053, -0.6026304364204407],\n",
    "            'translation': [-1609.8153076171875, 5177.3359375, 1537.896728515625],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5894251465797424, -0.7818877100944519, 0.13991211354732513, -0.14715361595153809],\n",
    "            'translation': [-1590.738037109375, -3854.1689453125, 1578.017578125],\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "class Human36mDataset(MocapDataset):\n",
    "    def __init__(self, path, remove_static_joints=False):\n",
    "        super().__init__(fps=50, skeleton=h36m_skeleton)\n",
    "        \n",
    "        self._cameras = copy.deepcopy(h36m_cameras_extrinsic_params)\n",
    "        for cameras in self._cameras.values():\n",
    "            for i, cam in enumerate(cameras):\n",
    "                cam.update(h36m_cameras_intrinsic_params[i])\n",
    "                for k, v in cam.items():\n",
    "                    if k not in ['id', 'res_w', 'res_h']:\n",
    "                        cam[k] = np.array(v, dtype='float32')\n",
    "                \n",
    "                # Normalize camera frame\n",
    "                cam['center'] = normalize_screen_coordinates(cam['center'], w=cam['res_w'], h=cam['res_h']).astype('float32')\n",
    "                cam['focal_length'] = cam['focal_length']/cam['res_w']*2\n",
    "                if 'translation' in cam:\n",
    "                    cam['translation'] = cam['translation']/1000 # mm to meters\n",
    "                \n",
    "                # Add intrinsic parameters vector\n",
    "                cam['intrinsic'] = np.concatenate((cam['focal_length'],\n",
    "                                                   cam['center'],\n",
    "                                                   cam['radial_distortion'],\n",
    "                                                   cam['tangential_distortion']))\n",
    "        \n",
    "        # Load serialized dataset\n",
    "        data = np.load(path, allow_pickle=True)['positions_3d'].item()\n",
    "        \n",
    "        self._data = {}\n",
    "        for subject, actions in data.items():\n",
    "            self._data[subject] = {}\n",
    "            for action_name, positions in actions.items():\n",
    "                self._data[subject][action_name] = {\n",
    "                    'positions': positions,\n",
    "                    'cameras': self._cameras[subject],\n",
    "                }\n",
    "                \n",
    "        if remove_static_joints:\n",
    "            # Bring the skeleton to 17 joints instead of the original 32\n",
    "            self.remove_joints([4, 5, 9, 10, 11, 16, 20, 21, 22, 23, 24, 28, 29, 30, 31])\n",
    "            \n",
    "            # Rewire shoulders to the correct parents\n",
    "            self._skeleton._parents[11] = 8\n",
    "            self._skeleton._parents[14] = 8\n",
    "            \n",
    "    def supports_semi_supervised(self):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6f1667-5447-422a-ab61-c3ea0b87a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import clip\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from einops import rearrange, repeat\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "    \n",
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \"\"\"Set requies_grad for all the networks.\n",
    "\n",
    "    Args:\n",
    "        nets (nn.Module | list[nn.Module]): A list of networks or a single\n",
    "            network.\n",
    "        requires_grad (bool): Whether the networks require gradients or not\n",
    "    \"\"\"\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., changedim=False, currentdim=0, depth=0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        \n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        \n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "        \n",
    "class GCN_Attention(nn.Module):\n",
    "    def __init__(self, dim = 256, spatial_adj = None, temporal_adj = None, proj_drop=0., mode='spatial'):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        self.spatial_adj = spatial_adj\n",
    "        self.temporal_adj = temporal_adj\n",
    "\n",
    "        self.mode = mode\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.U = nn.Linear(self.dim, self.dim)\n",
    "        self.V = nn.Linear(self.dim, self.dim)\n",
    "        \n",
    "    def normalize_digraph(self, adj):\n",
    "        b, n, c = adj.shape\n",
    "        vel_mag_degrees = adj.detach().sum(dim=-1)\n",
    "        deg_inv_sqrt = vel_mag_degrees ** -0.5\n",
    "        norm_deg_matrix = torch.eye(n).cuda()\n",
    "        norm_deg_matrix = norm_deg_matrix.view(1, n, n) * deg_inv_sqrt.view(b, n, 1)\n",
    "        norm_adj = torch.bmm(torch.bmm(norm_deg_matrix, adj), norm_deg_matrix)\n",
    "        return norm_adj\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == 'spatial':\n",
    "            adj = self.spatial_adj\n",
    "            adj = adj.repeat(x.shape[0], 1, 1)\n",
    "            norm_adj = self.normalize_digraph(adj)\n",
    "        elif self.mode == 'temporal':\n",
    "            adj = self.temporal_adj\n",
    "            adj = adj.repeat(x.shape[0], 1, 1)\n",
    "            norm_adj = self.normalize_digraph(adj)\n",
    "            norm_adj = norm_adj * (x @ x.transpose(-2,-1)).softmax(dim = -1)\n",
    "        else:\n",
    "            raise NotImplementedError(self.mode)\n",
    "\n",
    "        aggregate = norm_adj @ self.V(x) \n",
    "        x = aggregate + self.U(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., comb=False, vis=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim) \n",
    "\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.comb = comb\n",
    "        self.vis = vis\n",
    "\n",
    "    def forward(self, x, vis=False):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        if self.comb==True:\n",
    "            attn = (q.transpose(-2, -1) @ k) * self.scale\n",
    "        elif self.comb==False:\n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        if self.comb==True:\n",
    "            x = (attn @ v.transpose(-2, -1)).transpose(-2, -1)\n",
    "            x = rearrange(x, 'B H N C -> B N (H C)')\n",
    "        elif self.comb==False:\n",
    "            x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "        \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., attention=MHSA, qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, changedim=False, currentdim=0, depth=0, spatial_adj = None, temporal_adj = None,\n",
    "                 num_joints = 17, seq_len = 243, mode=\"spatial\", comb=False, vis=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.changedim = changedim\n",
    "        self.currentdim = currentdim\n",
    "        self.depth = depth\n",
    "        self.mode = mode\n",
    "        if self.changedim:\n",
    "            assert self.depth>0\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        \n",
    "        if attention == MHSA:\n",
    "            self.attn = attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, comb=comb, vis=vis)\n",
    "        elif attention == GCN_Attention:\n",
    "            self.attn = attention(dim, spatial_adj = spatial_adj, temporal_adj = temporal_adj, proj_drop=drop, mode=mode)\n",
    "        else:\n",
    "            raise NotImplementedError(attention)     \n",
    "        \n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        \n",
    "        if self.changedim and self.currentdim < self.depth//2:\n",
    "            self.reduction = nn.Conv1d(dim, dim//2, kernel_size=1)\n",
    "        elif self.changedim and depth > self.currentdim > self.depth//2:\n",
    "            self.improve = nn.Conv1d(dim, dim*2, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):               \n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        if self.changedim and self.currentdim < self.depth//2:\n",
    "            x = rearrange(x, 'b t c -> b c t')\n",
    "            x = self.reduction(x)\n",
    "            x = rearrange(x, 'b c t -> b t c')\n",
    "        elif self.changedim and self.depth > self.currentdim > self.depth//2:\n",
    "            x = rearrange(x, 'b t c -> b c t')\n",
    "            x = self.improve(x)\n",
    "            x = rearrange(x, 'b c t -> b t c')\n",
    "        return x\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "class StylizationBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim, time_embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, 2 * latent_dim),\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(latent_dim)\n",
    "        self.out_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            zero_module(nn.Linear(latent_dim, latent_dim)),\n",
    "        )\n",
    "\n",
    "    def forward(self, h, emb):\n",
    "        \"\"\"\n",
    "        h: B, T, D\n",
    "        emb: B, D\n",
    "        \"\"\"\n",
    "        B, T, D = h.shape\n",
    "        emb = emb.view(B, T, D)\n",
    "        # B, 1, 2D\n",
    "        emb_out = self.emb_layers(emb)[:,0:1,:]\n",
    "        # scale: B, 1, D / shift: B, 1, D\n",
    "        scale, shift = torch.chunk(emb_out, 2, dim=2)\n",
    "        h = self.norm(h) * (1 + scale) + shift\n",
    "        h = self.out_layers(h)\n",
    "        return h\n",
    "\n",
    "class TemporalCrossAttention(nn.Module):\n",
    "    def __init__(self, latent_dim, text_latent_dim, num_head, dropout, time_embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.norm = nn.LayerNorm(latent_dim)\n",
    "        self.text_norm = nn.LayerNorm(text_latent_dim)\n",
    "        self.query = nn.Linear(latent_dim, latent_dim)\n",
    "        self.key = nn.Linear(text_latent_dim, latent_dim)\n",
    "        self.value = nn.Linear(text_latent_dim, latent_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj_out = StylizationBlock(latent_dim, time_embed_dim, dropout)\n",
    "    \n",
    "    def forward(self, x, xf, emb):\n",
    "        \"\"\"\n",
    "        x: B, T, D\n",
    "        xf: B, N, L\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        N = xf.shape[1]\n",
    "        H = self.num_head\n",
    "        query = self.query(self.norm(x)).unsqueeze(2)\n",
    "        key = self.key(self.text_norm(xf)).unsqueeze(1)\n",
    "        key = key.repeat(int(B/key.shape[0]), 1, 1, 1)\n",
    "        query = query.view(B, T, H, -1)\n",
    "        key = key.view(B, N, H, -1)\n",
    "\n",
    "        attention = torch.einsum('bnhd,bmhd->bnmh', query, key) / math.sqrt(D // H)\n",
    "        weight = self.dropout(F.softmax(attention, dim=2))\n",
    "        value = self.value(self.text_norm(xf)).unsqueeze(1)\n",
    "        value = value.repeat(int(B/value.shape[0]), 1, 1, 1)\n",
    "        value = value.view(B, N, H, -1)\n",
    "        y = torch.einsum('bnmh,bmhd->bnhd', weight, value).reshape(B, T, D)\n",
    "        y = x + self.proj_out(y, emb)\n",
    "        return y   \n",
    "\n",
    "class Grap2Eq(nn.Module):\n",
    "    def __init__(self, num_frame=9, num_joints=17, in_chans=2, embed_dim_ratio=32, depth=4, spatial_adj = None, temporal_adj = None,\n",
    "                 num_heads=8, mlp_ratio=2., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2,  norm_layer=None, is_train=True):\n",
    "        \"\"\"    ##########hybrid_backbone=None, representation_size=None,\n",
    "        Args:\n",
    "            num_frame (int, tuple): input frame number\n",
    "            num_joints (int, tuple): joints number\n",
    "            in_chans (int): number of input channels, 2D joints have 2 channels: (x,y)\n",
    "            embed_dim_ratio (int): embedding dimension ratio\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        out_dim = 3\n",
    "        self.is_train=is_train\n",
    "        self.embed_dim_ratio = embed_dim_ratio\n",
    "        self.num_frame = num_frame\n",
    "\n",
    "        ### spatial patch embedding\n",
    "        self.Spatial_patch_to_embedding = nn.Linear(in_chans + 3, embed_dim_ratio)\n",
    "        self.Spatial_pos_embed = nn.Parameter(torch.zeros(1, num_joints, embed_dim_ratio))\n",
    "        self.Temporal_pos_embed = nn.Parameter(torch.zeros(1, num_frame, embed_dim_ratio))\n",
    "\n",
    "        self.prompt_learning = TemporalCrossAttention(embed_dim_ratio, embed_dim_ratio, num_heads, drop_rate, embed_dim_ratio)\n",
    "        \n",
    "        self.text_pre_proj = nn.Identity()\n",
    "        textTransEncoderLayer = nn.TransformerEncoderLayer(\n",
    "            d_model=512,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=512*4,\n",
    "            dropout=drop_rate,\n",
    "            activation=\"gelu\",\n",
    "            batch_first = True)\n",
    "        self.textTransEncoder = nn.TransformerEncoder(\n",
    "            textTransEncoderLayer,\n",
    "            num_layers=4)\n",
    "        self.text_ln = nn.LayerNorm(512)\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(512, 512)\n",
    "        )\n",
    "\n",
    "        self.clip_text, _ = clip.load('ViT-B/32', \"cpu\")\n",
    "        set_requires_grad(self.clip_text, False)\n",
    "\n",
    "        self.remain_len = 4\n",
    "\n",
    "        ctx_vectors_subject = torch.empty((7-self.remain_len), 512, dtype=self.clip_text.dtype)\n",
    "        nn.init.normal_(ctx_vectors_subject, std=0.02)\n",
    "        self.ctx_subject = nn.Parameter(ctx_vectors_subject)\n",
    "\n",
    "        ctx_vectors_verb = torch.empty((12-self.remain_len), 512, dtype=self.clip_text.dtype)\n",
    "        nn.init.normal_(ctx_vectors_verb, std=0.02)\n",
    "        self.ctx_verb = nn.Parameter(ctx_vectors_verb)\n",
    "\n",
    "        ctx_vectors_speed = torch.empty((10-self.remain_len), 512, dtype=self.clip_text.dtype)\n",
    "        nn.init.normal_(ctx_vectors_speed, std=0.02)\n",
    "        self.ctx_speed = nn.Parameter(ctx_vectors_speed)\n",
    "\n",
    "        ctx_vectors_head = torch.empty((10-self.remain_len), 512, dtype=self.clip_text.dtype)\n",
    "        nn.init.normal_(ctx_vectors_head, std=0.02)\n",
    "        self.ctx_head = nn.Parameter(ctx_vectors_head)\n",
    "        \n",
    "        ctx_vectors_body = torch.empty((10-self.remain_len), 512, dtype=self.clip_text.dtype)\n",
    "        nn.init.normal_(ctx_vectors_body, std=0.02)\n",
    "        self.ctx_body = nn.Parameter(ctx_vectors_body)\n",
    "        \n",
    "        ctx_vectors_arm = torch.empty((14-self.remain_len), 512, dtype=self.clip_text.dtype)\n",
    "        nn.init.normal_(ctx_vectors_arm, std=0.02)\n",
    "        self.ctx_arm = nn.Parameter(ctx_vectors_arm)\n",
    "\n",
    "        ctx_vectors_leg = torch.empty((14-self.remain_len), 512, dtype=self.clip_text.dtype)\n",
    "        nn.init.normal_(ctx_vectors_leg, std=0.02)\n",
    "        self.ctx_leg = nn.Parameter(ctx_vectors_leg)\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(embed_dim_ratio),\n",
    "            nn.Linear(embed_dim_ratio, embed_dim_ratio*2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim_ratio*2, embed_dim_ratio),\n",
    "        )\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.block_depth = depth\n",
    "\n",
    "        self.STEblocks = nn.ModuleList([\n",
    "            # Block: Attention Block\n",
    "            Block(num_joints = num_joints, seq_len = num_frame, attention=MHSA, mode=\"spatial\",\n",
    "                dim=embed_dim_ratio, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, spatial_adj = spatial_adj, temporal_adj = temporal_adj,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        self.TTEblocks = nn.ModuleList([\n",
    "            Block(num_joints = num_joints, seq_len = num_frame, attention=MHSA, mode=\"temporal\",\n",
    "                dim=embed_dim_ratio, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, spatial_adj = spatial_adj, temporal_adj = temporal_adj,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, changedim=False, currentdim=i+1, depth=depth)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        self.vel_spatial_blocks = nn.ModuleList([Block(num_joints = num_joints, seq_len = num_frame, attention=GCN_Attention, mode=\"spatial\", spatial_adj = spatial_adj, temporal_adj = temporal_adj,\n",
    "                dim=embed_dim_ratio, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "               \n",
    "        self.vel_temporal_blocks = nn.ModuleList([Block(num_joints = num_joints, seq_len = num_frame, attention=GCN_Attention, mode=\"temporal\", spatial_adj = spatial_adj, temporal_adj = temporal_adj,\n",
    "                dim=embed_dim_ratio, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "\n",
    "        self.spatial_vel_fuse = nn.ModuleList([nn.Linear(2, 2) for i in range(depth)])\n",
    "        self.temporal_vel_fuse = nn.ModuleList([nn.Linear(2, 2) for i in range(depth)])\n",
    "\n",
    "        self.Spatial_norm = norm_layer(embed_dim_ratio)\n",
    "        self.Temporal_norm = norm_layer(embed_dim_ratio)\n",
    "        \n",
    "        self.Spatial_vel_norm = norm_layer(embed_dim_ratio)\n",
    "        self.Temporal_vel_norm = norm_layer(embed_dim_ratio)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim_ratio),\n",
    "            nn.Linear(embed_dim_ratio , out_dim))\n",
    "\n",
    "    def STE_forward(self, x_2d, x_3d, t, xf_proj):\n",
    "        if self.is_train:\n",
    "            x = torch.cat((x_2d, x_3d), dim=-1)\n",
    "            b, f, n, c = x.shape\n",
    "            x = rearrange(x, 'b f n c  -> (b f) n c', )\n",
    "            x = self.Spatial_patch_to_embedding(x)\n",
    "            x += self.Spatial_pos_embed\n",
    "            time_embed = self.time_mlp(t)[:, None, None, :]\n",
    "            xf_proj = xf_proj.view(xf_proj.shape[0], 1, 1, xf_proj.shape[1])\n",
    "            time_embed = time_embed + xf_proj\n",
    "            time_embed = time_embed.repeat(1, f, n, 1)\n",
    "            time_embed = rearrange(time_embed, 'b f n c  -> (b f) n c', )\n",
    "            x += time_embed\n",
    "        else:\n",
    "            x_2d = x_2d[:,None].repeat(1,x_3d.shape[1],1,1,1)\n",
    "            x = torch.cat((x_2d, x_3d), dim=-1)\n",
    "            b, h, f, n, c = x.shape\n",
    "            x = rearrange(x, 'b h f n c  -> (b h f) n c', )\n",
    "            x = self.Spatial_patch_to_embedding(x)\n",
    "            x += self.Spatial_pos_embed\n",
    "            time_embed = self.time_mlp(t)[:, None, None, None, :]\n",
    "            xf_proj = xf_proj.view(xf_proj.shape[0], 1, 1, 1, xf_proj.shape[1])\n",
    "            time_embed = time_embed + xf_proj\n",
    "            time_embed = time_embed.repeat(1, h, f, n, 1)\n",
    "            time_embed = rearrange(time_embed, 'b h f n c  -> (b h f) n c', )\n",
    "            x += time_embed\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "        x = rearrange(x, '(b f) n cw -> (b n) f cw', f=f)\n",
    "        x += self.Temporal_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        x_ori = x.clone()\n",
    "        \n",
    "        x = rearrange(x, '(b n) f cw -> (b f) n cw', n=n)\n",
    "        x = self.STEblocks[0](x)\n",
    "        x = self.Spatial_norm(x)\n",
    "        \n",
    "        x_ori = rearrange(x_ori, '(b n) f cw -> b f n cw', n=n)\n",
    "        vel_3D = torch.zeros_like(x_ori).cuda()\n",
    "        vel_3D[:,1:,:, :] = x_ori[:,1:, :, :] - x_ori[:,:-1, :, :]\n",
    "        vel_3D[:, 0, :, :] = vel_3D[:,1, :, :]\n",
    "        \n",
    "        vel_3D = rearrange(vel_3D, 'b f n cw -> (b f) n cw', n=n)\n",
    "        vel_3D = self.vel_spatial_blocks[0](vel_3D)\n",
    "        vel_3D = self.Spatial_vel_norm(vel_3D)\n",
    "                \n",
    "        return x, vel_3D, time_embed\n",
    "\n",
    "    def TTE_foward(self, x, vel_3D):\n",
    "        assert len(x.shape) == 3, \"shape is equal to 3\"\n",
    "        x = self.TTEblocks[0](x)\n",
    "        x = self.Temporal_norm(x)\n",
    "        \n",
    "        vel_3D = self.vel_temporal_blocks[0](vel_3D)\n",
    "        vel_3D = self.Temporal_vel_norm(vel_3D)\n",
    "        \n",
    "        return x, vel_3D\n",
    "\n",
    "    def ST_w_vel_ref(self, x, vel_3D, x_2d, x_3d):\n",
    "        assert len(x.shape)==4, \"shape is equal to 4\"\n",
    "        if self.is_train:\n",
    "            b, f, n, c = x_2d.shape\n",
    "        else:\n",
    "            b, h, f, n, c = x_3d.shape\n",
    " \n",
    "        for i in range(1, self.block_depth):\n",
    "            x = rearrange(x, 'b f n cw -> (b f) n cw',)\n",
    "            vel_3D = rearrange(vel_3D, 'b f n cw -> (b f) n cw',) \n",
    "                  \n",
    "            x = self.STEblocks[i](x)\n",
    "            x = self.Spatial_norm(x)\n",
    "            \n",
    "            vel_3D = self.vel_spatial_blocks[i](vel_3D)\n",
    "            vel_3D = self.Spatial_vel_norm(vel_3D)\n",
    "            \n",
    "            x = rearrange(x, '(b f) n cw -> b f n cw', f=f)\n",
    "            vel_3D = rearrange(vel_3D, '(b f) n cw -> b f n cw', f=f)\n",
    "            \n",
    "            x, vel_3D = self.update_coord_w_fused_vel(x, vel_3D, i, 'spatial')            \n",
    "            \n",
    "            x = rearrange(x, 'b f n cw -> (b n) f cw',)\n",
    "            vel_3D = rearrange(vel_3D, 'b f n cw -> (b n) f cw',)\n",
    "\n",
    "            x = self.TTEblocks[i](x)\n",
    "            x = self.Temporal_norm(x)\n",
    "            \n",
    "            vel_3D = self.vel_temporal_blocks[i](vel_3D)\n",
    "            vel_3D = self.Temporal_vel_norm(vel_3D)\n",
    "            \n",
    "            x = rearrange(x, '(b n) f cw -> b f n cw', n=n)\n",
    "            vel_3D = rearrange(vel_3D, '(b n) f cw -> b f n cw', n=n)\n",
    "            \n",
    "            x, vel_3D = self.update_coord_w_fused_vel(x, vel_3D, i, 'temporal')\n",
    "     \n",
    "        return x\n",
    "        \n",
    "    def encode_text(self, text, pre_text_tensor):\n",
    "        with torch.no_grad():\n",
    "            x = self.clip_text.token_embedding(text).type(self.clip_text.dtype)\n",
    "            pre_text_tensor = self.clip_text.token_embedding(pre_text_tensor).type(self.clip_text.dtype)\n",
    "            \n",
    "        learnable_prompt_subject = self.ctx_subject\n",
    "        learnable_prompt_subject = learnable_prompt_subject.view(1, self.ctx_subject.shape[0], self.ctx_subject.shape[1])\n",
    "        learnable_prompt_subject = learnable_prompt_subject.repeat(x.shape[0], 1, 1)\n",
    "        learnable_prompt_subject = torch.cat((learnable_prompt_subject, pre_text_tensor[:, 0, :self.remain_len, :]), dim=1)\n",
    "\n",
    "        learnable_prompt_verb = self.ctx_verb\n",
    "        learnable_prompt_verb = learnable_prompt_verb.view(1, self.ctx_verb.shape[0], self.ctx_verb.shape[1])\n",
    "        learnable_prompt_verb = learnable_prompt_verb.repeat(x.shape[0], 1, 1)\n",
    "        learnable_prompt_verb = torch.cat((learnable_prompt_verb, x[:, :self.remain_len, :]), dim=1)\n",
    "\n",
    "        learnable_prompt_speed = self.ctx_speed\n",
    "        learnable_prompt_speed = learnable_prompt_speed.view(1, self.ctx_speed.shape[0], self.ctx_speed.shape[1])\n",
    "        learnable_prompt_speed = learnable_prompt_speed.repeat(x.shape[0], 1, 1)\n",
    "        learnable_prompt_speed = torch.cat((learnable_prompt_speed, pre_text_tensor[:, 1, :self.remain_len, :]), dim=1)\n",
    "\n",
    "        learnable_prompt_head = self.ctx_head\n",
    "        learnable_prompt_head = learnable_prompt_head.view(1, self.ctx_head.shape[0], self.ctx_head.shape[1])\n",
    "        learnable_prompt_head = learnable_prompt_head.repeat(x.shape[0], 1, 1)\n",
    "        learnable_prompt_head = torch.cat((learnable_prompt_head, pre_text_tensor[:, 2, :self.remain_len, :]), dim=1)\n",
    "\n",
    "        learnable_prompt_body = self.ctx_body\n",
    "        learnable_prompt_body = learnable_prompt_body.view(1, self.ctx_body.shape[0], self.ctx_body.shape[1])\n",
    "        learnable_prompt_body = learnable_prompt_body.repeat(x.shape[0], 1, 1)\n",
    "        learnable_prompt_body = torch.cat((learnable_prompt_body, pre_text_tensor[:, 3, :self.remain_len, :]), dim=1)\n",
    "\n",
    "        learnable_prompt_arm = self.ctx_arm\n",
    "        learnable_prompt_arm = learnable_prompt_arm.view(1, self.ctx_arm.shape[0], self.ctx_arm.shape[1])\n",
    "        learnable_prompt_arm = learnable_prompt_arm.repeat(x.shape[0], 1, 1)\n",
    "        learnable_prompt_arm = torch.cat((learnable_prompt_arm, pre_text_tensor[:, 4, :self.remain_len, :]), dim=1)\n",
    "\n",
    "        learnable_prompt_leg = self.ctx_leg\n",
    "        learnable_prompt_leg = learnable_prompt_leg.view(1, self.ctx_leg.shape[0], self.ctx_leg.shape[1])\n",
    "        learnable_prompt_leg = learnable_prompt_leg.repeat(x.shape[0], 1, 1)\n",
    "        learnable_prompt_leg = torch.cat((learnable_prompt_leg, pre_text_tensor[:, 5, :self.remain_len, :]), dim=1)\n",
    "\n",
    "        x = torch.cat((learnable_prompt_subject, learnable_prompt_verb, learnable_prompt_speed, learnable_prompt_head, learnable_prompt_body, learnable_prompt_arm, learnable_prompt_leg), dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = x + self.clip_text.positional_embedding.type(self.clip_text.dtype)\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "            x = self.clip_text.transformer(x)\n",
    "            x = self.clip_text.ln_final(x).type(self.clip_text.dtype)\n",
    "\n",
    "        x = self.text_pre_proj(x)\n",
    "        xf_out = self.textTransEncoder(x)\n",
    "        xf_out = self.text_ln(xf_out)\n",
    "        xf_proj = self.text_proj(xf_out[text.argmax(dim=-1), torch.arange(xf_out.shape[1])])\n",
    "        # B, T, D\n",
    "        xf_out = xf_out.permute(1, 0, 2)\n",
    "        return xf_proj, xf_out\n",
    "        \n",
    "    def update_coord_w_fused_vel(self, x, vel_3D, i, mode):        \n",
    "        b, f, n, c = x.shape\n",
    "        vel_3D_1 = torch.zeros_like(x).cuda()\n",
    "        vel_3D_1[:,1:, :, :] = x[:,1:,:, :] - x[:,:-1, :, :]\n",
    "        vel_3D_1[:, 0, :, :] = vel_3D_1[:,1, :, :]\n",
    "            \n",
    "        vel_3D_fused = torch.cat((vel_3D, vel_3D_1), dim=-1).reshape(b, f, n,self.embed_dim_ratio, 2)\n",
    "        \n",
    "        if mode == 'spatial':     \n",
    "            alpha = self.spatial_vel_fuse[i] (vel_3D_fused)\n",
    "        elif mode == 'temporal':\n",
    "            alpha = self.temporal_vel_fuse[i] (vel_3D_fused)\n",
    "                    \n",
    "        alpha = alpha.softmax(dim = -1)\n",
    "        vel_3D_fused = alpha[:,:, :,:, 0] * vel_3D + alpha[:,:, :,:, 1] * vel_3D_1\n",
    "            \n",
    "        x[:,1:,:, :] = x[:,:-1, :, :] + vel_3D_fused[:,1:, :, :]              \n",
    "      \n",
    "        return x, vel_3D_fused\n",
    "\n",
    "    def forward(self, x_2d, x_3d, t, text, pre_text_tensor):\n",
    "        if self.is_train:\n",
    "            b, f, n, c = x_2d.shape\n",
    "        else:\n",
    "            b, h, f, n, c = x_3d.shape\n",
    "        \n",
    "        xf_proj, xf_out = self.encode_text(text, pre_text_tensor)\n",
    "\n",
    "        x, vel_3D, time_embed = self.STE_forward(x_2d, x_3d, t, xf_proj)\n",
    "        \n",
    "        x = rearrange(x, '(b f) n cw -> (b n) f cw', f=f)\n",
    "        vel_3D = rearrange(vel_3D, '(b f) n cw -> (b n) f cw', f=f)\n",
    "        \n",
    "        x = self.prompt_learning(x, xf_out, time_embed)\n",
    "        \n",
    "        x = rearrange(x, '(b n) f cw -> b f n cw', n = n)\n",
    "        vel_3D = rearrange(vel_3D, '(b n) f cw -> b f n cw', n=n)\n",
    "        \n",
    "        x, vel_3D = self.update_coord_w_fused_vel(x, vel_3D, 0, 'spatial')\n",
    "        \n",
    "        x = rearrange(x, 'b f n cw -> (b n) f cw',)\n",
    "        vel_3D = rearrange(vel_3D, 'b f n cw -> (b n) f cw',)\n",
    "\n",
    "        x, vel_3D = self.TTE_foward(x, vel_3D)\n",
    "        \n",
    "        x = rearrange(x, '(b n) f cw -> b f n cw', n=n)\n",
    "        vel_3D = rearrange(vel_3D, '(b n) f cw -> b f n cw', n=n)\n",
    "        \n",
    "        x, vel_3D = self.update_coord_w_fused_vel(x, vel_3D, 0, 'temporal')\n",
    "\n",
    "        x = self.ST_w_vel_ref(x, vel_3D, x_2d, x_3d)\n",
    "\n",
    "        x = self.head(x)\n",
    "\n",
    "        if self.is_train:\n",
    "            x = x.view(b, f, n, -1)\n",
    "        else:\n",
    "            x = x.view(b, h, f, n, -1)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17bc025-b65a-492a-b69a-9d8c9c28d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import List\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from time import time\n",
    "\n",
    "\n",
    "# from common. import *\n",
    "# from common.Grap2Eq import *\n",
    "\n",
    "__all__ = [\"D3DP_Grap2Eq\"]\n",
    "\n",
    "ModelPrediction = namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    \"\"\"extract the appropriate  t  index for a batch of indices\"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "\n",
    "class D3DP_Grap2Eq(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement Grap2Eq\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, joints_left, joints_right, num_joints=17, is_train=True, num_proposals=1, sampling_timesteps=1, spatial_adj = None, temporal_adj = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.frames = 243\n",
    "        self.num_proposals = num_proposals\n",
    "        self.flip = False\n",
    "        self.joints_left = joints_left\n",
    "        self.joints_right = joints_right\n",
    "        self.is_train = is_train\n",
    "        self.num_joints = num_joints\n",
    "        self.device = 'cuda'\n",
    "\n",
    "        # build diffusion\n",
    "        timesteps = 1000\n",
    "        sampling_timesteps = sampling_timesteps\n",
    "        self.objective = 'pred_x0'\n",
    "        betas = cosine_beta_schedule(timesteps)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        self.sampling_timesteps = default(sampling_timesteps, timesteps)\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = 1.\n",
    "        self.self_condition = False\n",
    "        self.scale = 1.0\n",
    "        self.box_renewal = True\n",
    "        self.use_ensemble = True\n",
    "\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        self.register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "        self.register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min=1e-20)))\n",
    "        self.register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        self.register_buffer('posterior_mean_coef2',\n",
    "                             (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "\n",
    "        # Build Dynamic Head.\n",
    "        drop_path_rate=0\n",
    "        if is_train:\n",
    "            drop_path_rate=0.1\n",
    "\n",
    "        self.pose_estimator = Grap2Eq(num_frame=self.frames, num_joints=num_joints, in_chans=2, embed_dim_ratio=512, depth=8,\n",
    "        num_heads=8, mlp_ratio=2., qkv_bias=True, qk_scale=None,drop_path_rate=drop_path_rate, is_train=is_train, spatial_adj = spatial_adj, temporal_adj = temporal_adj)        \n",
    "\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        return (\n",
    "                (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) /\n",
    "                extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def model_predictions(self, x, inputs_2d, t, input_text, pre_text_tensor):\n",
    "        x_t = torch.clamp(x, min=-1.1 * self.scale, max=1.1*self.scale)\n",
    "        x_t = x_t / self.scale\n",
    "        pred_pose = self.pose_estimator(inputs_2d, x_t, t, input_text, pre_text_tensor)\n",
    "\n",
    "        x_start = pred_pose\n",
    "        x_start = x_start * self.scale\n",
    "        x_start = torch.clamp(x_start, min=-1.1 * self.scale, max=1.1*self.scale)\n",
    "        pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def model_predictions_fliping(self, x, inputs_2d, inputs_2d_flip, t, input_text, pre_text_tensor):\n",
    "        x_t = torch.clamp(x, min=-1.1 * self.scale, max=1.1*self.scale)\n",
    "        x_t = x_t / self.scale\n",
    "        x_t_flip = x_t.clone()\n",
    "        x_t_flip[:, :, :, :, 0] *= -1\n",
    "        x_t_flip[:, :, :, self.joints_left + self.joints_right] = x_t_flip[:, :, :, self.joints_right + self.joints_left]\n",
    "\n",
    "        pred_pose = self.pose_estimator(inputs_2d, x_t, t, input_text, pre_text_tensor)\n",
    "        pred_pose_flip = self.pose_estimator(inputs_2d_flip, x_t_flip, t, input_text, pre_text_tensor)\n",
    "\n",
    "        pred_pose_flip[:, :, :, :, 0] *= -1\n",
    "        pred_pose_flip[:, :, :, self.joints_left + self.joints_right] = pred_pose_flip[:, :, :,\n",
    "                                                                      self.joints_right + self.joints_left]\n",
    "        pred_pose = (pred_pose + pred_pose_flip) / 2\n",
    "\n",
    "        x_start = pred_pose\n",
    "        x_start = x_start * self.scale\n",
    "        x_start = torch.clamp(x_start, min=-1.1 * self.scale, max=1.1*self.scale)\n",
    "        pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "        pred_noise = pred_noise.float()\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, inputs_2d, inputs_3d, input_text, pre_text_tensor, clip_denoised=True, do_postprocess=True):\n",
    "        batch = inputs_2d.shape[0]\n",
    "        shape = (batch, self.num_proposals, self.frames, self.num_joints, 3)\n",
    "        total_timesteps, sampling_timesteps, eta, objective = self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps=sampling_timesteps + 1)\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:]))\n",
    "        \n",
    "        # original random noise\n",
    "        img = torch.randn(shape, device=self.device)\n",
    "\n",
    "        ensemble_score, ensemble_label, ensemble_coord = [], [], []\n",
    "        x_start = None\n",
    "        preds_all=[]\n",
    "        for time, time_next in time_pairs:\n",
    "            time_cond = torch.full((batch,), time, device=self.device, dtype=torch.long)\n",
    "\n",
    "\n",
    "            preds = self.model_predictions(img, inputs_2d, time_cond, input_text, pre_text_tensor)\n",
    "            pred_noise, x_start = preds.pred_noise, preds.pred_x_start\n",
    "            preds_all.append(x_start)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "        return preds_all\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample_flip(self, inputs_2d, inputs_3d, input_text, pre_text_tensor, clip_denoised=True, do_postprocess=True, input_2d_flip=None):\n",
    "        batch = inputs_2d.shape[0]\n",
    "        shape = (batch, self.num_proposals, self.frames, self.num_joints, 3)\n",
    "        total_timesteps, sampling_timesteps, eta, objective = self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps=sampling_timesteps + 1)\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:]))\n",
    "\n",
    "        img = torch.randn(shape, device='cuda')\n",
    "\n",
    "        x_start = None\n",
    "        preds_all = []\n",
    "        for time, time_next in time_pairs:\n",
    "            time_cond = torch.full((batch,), time, dtype=torch.long).cuda()\n",
    "\n",
    "\n",
    "            preds = self.model_predictions_fliping(img, inputs_2d, input_2d_flip, time_cond, input_text, pre_text_tensor)\n",
    "            pred_noise, x_start = preds.pred_noise, preds.pred_x_start\n",
    "\n",
    "            preds_all.append(x_start)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "        return torch.stack(preds_all, dim=1)\n",
    "\n",
    "\n",
    "    # forward diffusion\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def forward(self, input_2d, input_3d, input_text, pre_text_tensor, input_2d_flip=None):\n",
    "        # Prepare Proposals.\n",
    "        if not self.is_train:\n",
    "            if self.flip:\n",
    "                results = self.ddim_sample_flip(input_2d, input_3d, input_text, pre_text_tensor, input_2d_flip=input_2d_flip)\n",
    "            else:\n",
    "                results = self.ddim_sample(input_2d, input_3d, input_text, pre_text_tensor)\n",
    "            return results\n",
    "\n",
    "        if self.is_train:\n",
    "\n",
    "            x_poses, noises, t = self.prepare_targets(input_3d)\n",
    "            x_poses = x_poses.float()\n",
    "            t = t.squeeze(-1)\n",
    "\n",
    "            pred_pose = self.pose_estimator(input_2d, x_poses, t, input_text, pre_text_tensor)\n",
    "\n",
    "            return pred_pose\n",
    "\n",
    "\n",
    "    def prepare_diffusion_concat(self, pose_3d):\n",
    "\n",
    "        t = torch.randint(0, self.num_timesteps, (1,), device='cuda').long()\n",
    "        noise = torch.randn(self.frames, self.num_joints, 3, device='cuda')\n",
    "\n",
    "        x_start = pose_3d\n",
    "\n",
    "        x_start = x_start * self.scale\n",
    "\n",
    "        # noise sample\n",
    "        x = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "\n",
    "        x = torch.clamp(x, min= -1.1 * self.scale, max= 1.1*self.scale)\n",
    "        x = x / self.scale\n",
    "\n",
    "\n",
    "        return x, noise, t\n",
    "\n",
    "    def prepare_targets(self, targets):\n",
    "        diffused_poses = []\n",
    "        noises = []\n",
    "        ts = []\n",
    "        for i in range(0,targets.shape[0]):\n",
    "            targets_per_sample = targets[i]\n",
    "\n",
    "            d_poses, d_noise, d_t = self.prepare_diffusion_concat(targets_per_sample)\n",
    "            diffused_poses.append(d_poses)\n",
    "            noises.append(d_noise)\n",
    "            ts.append(d_t)\n",
    "        \n",
    "\n",
    "        return torch.stack(diffused_poses), torch.stack(noises), torch.stack(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9cdd434-c4b6-4f7e-846a-85d5f48380b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2D detections...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math \n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#cudnn.benchmark = True       \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print('Loading 2D detections...')\n",
    "keypoints = np.load('/data_2d_h36m_cpn_ft_h36m_dbb.npz', allow_pickle=True)\n",
    "data_path = '/data_3d_h36m.npz'\n",
    "dataset = Human36mDataset(data_path)\n",
    "\n",
    "keypoints_metadata = keypoints['metadata'].item()\n",
    "keypoints_symmetry = keypoints_metadata['keypoints_symmetry']\n",
    "kps_left, kps_right = list(keypoints_symmetry[0]), list(keypoints_symmetry[1])\n",
    "joints_left, joints_right = list(dataset.skeleton().joints_left()), list(dataset.skeleton().joints_right())\n",
    "pad = (243 -1) // 2 # Padding on each side\n",
    "receptive_field = 243\n",
    "\n",
    "class UnchunkedGenerator_Seq:\n",
    "    \"\"\"\n",
    "    Non-batched data generator, used for testing.\n",
    "    Sequences are returned one at a time (i.e. batch size = 1), without chunking.\n",
    "    \n",
    "    If data augmentation is enabled, the batches contain two sequences (i.e. batch size = 2),\n",
    "    the second of which is a mirrored version of the first.\n",
    "    \n",
    "    Arguments:\n",
    "    cameras -- list of cameras, one element for each video (optional, used for semi-supervised training)\n",
    "    poses_3d -- list of ground-truth 3D poses, one element for each video (optional, used for supervised training)\n",
    "    poses_2d -- list of input 2D keypoints, one element for each video\n",
    "    pad -- 2D input padding to compensate for valid convolutions, per side (depends on the receptive field)\n",
    "    causal_shift -- asymmetric padding offset when causal convolutions are used (usually 0 or \"pad\")\n",
    "    augment -- augment the dataset by flipping poses horizontally\n",
    "    kps_left and kps_right -- list of left/right 2D keypoints if flipping is enabled\n",
    "    joints_left and joints_right -- list of left/right 3D joints if flipping is enabled\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cameras, poses_3d, poses_2d, action, pad=0, causal_shift=0,\n",
    "                 augment=False, kps_left=None, kps_right=None, joints_left=None, joints_right=None):\n",
    "                 \n",
    "        assert poses_3d is None or len(poses_3d) == len(poses_2d)\n",
    "        assert cameras is None or len(cameras) == len(poses_2d)\n",
    "\n",
    "        self.augment = False\n",
    "        self.kps_left = kps_left\n",
    "        self.kps_right = kps_right\n",
    "        self.joints_left = joints_left\n",
    "        self.joints_right = joints_right\n",
    "        \n",
    "        self.pad = pad\n",
    "        self.causal_shift = causal_shift\n",
    "        self.cameras = [] if cameras is None else cameras\n",
    "        self.poses_3d = [] if poses_3d is None else poses_3d\n",
    "        self.action = action\n",
    "        self.poses_2d = poses_2d\n",
    "        \n",
    "    def num_frames(self):\n",
    "        count = 0\n",
    "        for p in self.poses_2d:\n",
    "            count += p.shape[0]\n",
    "        return count\n",
    "\n",
    "    def batch_num(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def augment_enabled(self):\n",
    "        return self.augment\n",
    "    \n",
    "    def set_augment(self, augment):\n",
    "        self.augment = augment\n",
    "    \n",
    "    def next_epoch(self):\n",
    "        for seq_cam, seq_3d, seq_2d, seq_act in zip_longest(self.cameras, self.poses_3d, self.poses_2d, self.action):\n",
    "            batch_cam = None if seq_cam is None else np.expand_dims(seq_cam, axis=0)\n",
    "            batch_3d = None if seq_3d is None else np.expand_dims(seq_3d, axis=0)\n",
    "            batch_2d = None if seq_2d is None else np.expand_dims(seq_2d, axis=0)\n",
    "            batch_act = None if seq_act is None else np.expand_dims(seq_act, axis=0)\n",
    "            batch_act = batch_act.astype(object)\n",
    "            if self.augment:\n",
    "                # Append flipped version\n",
    "                if batch_cam is not None:\n",
    "                    batch_cam = np.concatenate((batch_cam, batch_cam), axis=0)\n",
    "                    batch_cam[1, 2] *= -1\n",
    "                    batch_cam[1, 7] *= -1\n",
    "                \n",
    "                if batch_3d is not None:\n",
    "                    batch_3d = np.concatenate((batch_3d, batch_3d), axis=0)\n",
    "                    batch_3d[1, :, :, 0] *= -1\n",
    "                    batch_3d[1, :, self.joints_left + self.joints_right] = batch_3d[1, :, self.joints_right + self.joints_left]\n",
    "\n",
    "                batch_2d = np.concatenate((batch_2d, batch_2d), axis=0)\n",
    "                batch_2d[1, :, :, 0] *= -1\n",
    "                batch_2d[1, :, self.kps_left + self.kps_right] = batch_2d[1, :, self.kps_right + self.kps_left]\n",
    "            yield batch_cam, batch_3d, batch_2d, batch_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4087cac1-e34e-4c27-af9f-1b6e3e06961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluate checkpoint /Grap2Eq/epoch_101.bin\n",
      "This model was trained for 101 epochs\n"
     ]
    }
   ],
   "source": [
    "def encode_text(text):\n",
    "    with torch.no_grad():\n",
    "        text = clip.tokenize(text, truncate=True).cuda()\n",
    "        return text\n",
    "        \n",
    "pre_text_information = [\n",
    "    \"a person\",\n",
    "    \"speed\",\n",
    "    \"head\",\n",
    "    \"body\",\n",
    "    \"arm\",\n",
    "    \"leg\",\n",
    "]\n",
    "\n",
    "pre_text_tensor = []\n",
    "for i in pre_text_information:\n",
    "    tmp_text = encode_text(i)\n",
    "    pre_text_tensor.append(tmp_text)\n",
    "\n",
    "pre_text_tensor = torch.cat(pre_text_tensor, dim=0)\n",
    "\n",
    "#Connections for Human3.6M dataset\n",
    "connections = {0: [1,4], 1: [0,2], 2: [1,3], 3:[2], 4: [0,5], 5: [4,6], 6:[5], 7:[0,8], 8:[7,9,11,14], 9:[8,10], 10:[9], 11:[8,12], 12:[11,13], 13: [12], 14: [8,15], 15: [14,16], 16: [15]} \n",
    "\n",
    "def init_spatial_adj(connections, num_joints):\n",
    "    adj = torch.zeros((num_joints, num_joints)).cuda()\n",
    "\n",
    "    for i in range(num_joints):\n",
    "        connected_map = connections[i]\n",
    "        adj[i, i] = 1/len(connected_map)\n",
    "        for j in connected_map:\n",
    "            adj[i, j] = 1/math.sqrt(len(connected_map)*len(connections[j]))\n",
    "    return adj\n",
    "        \n",
    "def init_temporal_adj(seq_len, alpha, beta):\n",
    "    \"\"\"Connects each joint to itself and the neighbour joints using logistic decay function. The further the frame, the less attention\"\"\"\n",
    "    adj = torch.zeros((seq_len, seq_len)).cuda()\n",
    "\n",
    "    for i in range(seq_len):    \n",
    "        for j in range(seq_len):                                  \n",
    "            adj[i,j] = 1/(1+math.exp(alpha* (abs(i-j)-beta)))                    \n",
    "    return adj\n",
    "\n",
    "spatial_adj = init_spatial_adj(connections, 17)\n",
    "temporal_adj = init_temporal_adj(243, 0.25, 5)\n",
    "\n",
    "model_pos = D3DP_Grap2Eq(joints_left, joints_right, 17, is_train=False, num_proposals=1, sampling_timesteps=1, spatial_adj = spatial_adj, temporal_adj = temporal_adj)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_pos = nn.DataParallel(model_pos)\n",
    "    model_pos = model_pos.cuda()\n",
    "\n",
    "    model_eval = model_pos\n",
    "    # load best checkpoint\n",
    "    chk_file_path = \"/Inference/pretrained_models/epoch_101.bin\"\n",
    "    print('Loading evaluate checkpoint', chk_file_path)\n",
    "    checkpoint = torch.load(chk_file_path, map_location=lambda storage, loc: storage)\n",
    "    print('This model was trained for {} epochs'.format(checkpoint['epoch']))\n",
    "    model_eval.load_state_dict(checkpoint['model_pos'])\n",
    "    model_eval.eval()\n",
    "\n",
    "def eval_data_prepare(receptive_field, inputs_2d, inputs_3d):\n",
    "\n",
    "    assert inputs_2d.shape[:-1] == inputs_3d.shape[:-1], \"2d and 3d inputs shape must be same! \"+str(inputs_2d.shape)+str(inputs_3d.shape)\n",
    "    inputs_2d_p = torch.squeeze(inputs_2d)\n",
    "    inputs_3d_p = torch.squeeze(inputs_3d)\n",
    "\n",
    "    if inputs_2d_p.shape[0] / receptive_field > inputs_2d_p.shape[0] // receptive_field: \n",
    "        out_num = inputs_2d_p.shape[0] // receptive_field+1\n",
    "    elif inputs_2d_p.shape[0] / receptive_field == inputs_2d_p.shape[0] // receptive_field:\n",
    "        out_num = inputs_2d_p.shape[0] // receptive_field\n",
    "\n",
    "    eval_input_2d = torch.empty(out_num, receptive_field, inputs_2d_p.shape[1], inputs_2d_p.shape[2])\n",
    "    eval_input_3d = torch.empty(out_num, receptive_field, inputs_3d_p.shape[1], inputs_3d_p.shape[2])\n",
    "\n",
    "    for i in range(out_num-1):\n",
    "        eval_input_2d[i,:,:,:] = inputs_2d_p[i*receptive_field:i*receptive_field+receptive_field,:,:]\n",
    "        eval_input_3d[i,:,:,:] = inputs_3d_p[i*receptive_field:i*receptive_field+receptive_field,:,:]\n",
    "    if inputs_2d_p.shape[0] < receptive_field:\n",
    "        from torch.nn import functional as F\n",
    "        pad_right = receptive_field-inputs_2d_p.shape[0]\n",
    "        inputs_2d_p = rearrange(inputs_2d_p, 'b f c -> f c b')\n",
    "        inputs_2d_p = F.pad(inputs_2d_p, (0,pad_right), mode='replicate')\n",
    "        inputs_2d_p = rearrange(inputs_2d_p, 'f c b -> b f c')\n",
    "    if inputs_3d_p.shape[0] < receptive_field:\n",
    "        pad_right = receptive_field-inputs_3d_p.shape[0]\n",
    "        inputs_3d_p = rearrange(inputs_3d_p, 'b f c -> f c b')\n",
    "        inputs_3d_p = F.pad(inputs_3d_p, (0,pad_right), mode='replicate')\n",
    "        inputs_3d_p = rearrange(inputs_3d_p, 'f c b -> b f c')\n",
    "    eval_input_2d[-1,:,:,:] = inputs_2d_p[-receptive_field:,:,:]\n",
    "    eval_input_3d[-1,:,:,:] = inputs_3d_p[-receptive_field:,:,:]\n",
    "\n",
    "    return eval_input_2d, eval_input_3d\n",
    "\n",
    "    \n",
    "def evaluate(test_generator, action=None, return_predictions=False, use_trajectory_model=False, newmodel=None):\n",
    "    with torch.no_grad():\n",
    "        N = 0\n",
    "        iteration = 10\n",
    "\n",
    "        # quickdebug=args.debug\n",
    "        for cam, batch, batch_2d, batch_act in test_generator.next_epoch():\n",
    "            for i in range(batch_act.shape[0]):\n",
    "                batch_act[i] = batch_act[i].split(\" \")[0]\n",
    "            input_text = []\n",
    "            for i in batch_act:\n",
    "                tmp_text = encode_text(i)\n",
    "                input_text.append(tmp_text)\n",
    "            input_text = torch.cat(input_text, dim=0)\n",
    "\n",
    "            inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            temp = np.zeros((1, inputs_2d.shape[1], 17, 3))\n",
    "            inputs_3d = torch.from_numpy(temp.astype('float32'))\n",
    "            cam = torch.from_numpy(cam.astype('float32'))\n",
    "\n",
    "\n",
    "            ##### apply test-time-augmentation (following D3DP)\n",
    "            inputs_2d_flip = inputs_2d.clone()\n",
    "            inputs_2d_flip [:, :, :, 0] *= -1\n",
    "            inputs_2d_flip[:, :, kps_left + kps_right,:] = inputs_2d_flip[:, :, kps_right + kps_left,:]\n",
    "\n",
    "            ##### convert size\n",
    "            inputs_3d_p = inputs_3d\n",
    "            if newmodel is not None:\n",
    "                def eval_data_prepare_pf(receptive_field, inputs_2d, inputs_3d):\n",
    "                    inputs_2d_p = torch.squeeze(inputs_2d)\n",
    "                    inputs_3d_p = inputs_3d.permute(1,0,2,3)\n",
    "                    padding = int(receptive_field//2)\n",
    "                    inputs_2d_p = rearrange(inputs_2d_p, 'b f c -> f c b')\n",
    "                    inputs_2d_p = F.pad(inputs_2d_p, (padding,padding), mode='replicate')\n",
    "                    inputs_2d_p = rearrange(inputs_2d_p, 'f c b -> b f c')\n",
    "                    out_num = inputs_2d_p.shape[0] - receptive_field + 1\n",
    "                    eval_input_2d = torch.empty(out_num, receptive_field, inputs_2d_p.shape[1], inputs_2d_p.shape[2])\n",
    "                    for i in range(out_num):\n",
    "                        eval_input_2d[i,:,:,:] = inputs_2d_p[i:i+receptive_field, :, :]\n",
    "                    return eval_input_2d, inputs_3d_p\n",
    "                \n",
    "                inputs_2d, inputs_3d = eval_data_prepare_pf(81, inputs_2d, inputs_3d_p)\n",
    "                inputs_2d_flip, _ = eval_data_prepare_pf(81, inputs_2d_flip, inputs_3d_p)\n",
    "            else:\n",
    "                inputs_2d, inputs_3d = eval_data_prepare(receptive_field, inputs_2d, inputs_3d_p)\n",
    "                inputs_2d_flip, _ = eval_data_prepare(receptive_field, inputs_2d_flip, inputs_3d_p)\n",
    "\n",
    "            input_text = input_text.repeat(int(inputs_2d.shape[0]/input_text.shape[0]), 1)\n",
    "            pre_text_tensor_valid = pre_text_tensor.unsqueeze(dim=0)\n",
    "            pre_text_tensor_valid = pre_text_tensor_valid.repeat(input_text.shape[0], 1, 1)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_2d = inputs_2d.cuda()\n",
    "                inputs_2d_flip = inputs_2d_flip.cuda()\n",
    "                inputs_3d = inputs_3d.cuda()\n",
    "                pre_text_tensor_valid = pre_text_tensor_valid.cuda()\n",
    "                input_text = input_text.cuda()\n",
    "                cam = cam.cuda()\n",
    "\n",
    "            inputs_traj = inputs_3d[:, :, :1].clone()\n",
    "            inputs_3d[:, :, 0] = 0\n",
    "\n",
    "            bs = 1\n",
    "            total_batch = (inputs_3d.shape[0] + bs - 1) // bs\n",
    "\n",
    "            for batch_cnt in range(total_batch):\n",
    "\n",
    "                if (batch_cnt + 1) * bs > inputs_3d.shape[0]:\n",
    "                    inputs_2d_single = inputs_2d[batch_cnt * bs:]\n",
    "                    inputs_2d_flip_single = inputs_2d_flip[batch_cnt * bs:]\n",
    "                    inputs_3d_single = inputs_3d[batch_cnt * bs:]\n",
    "                    inputs_traj_single = inputs_traj[batch_cnt * bs:]\n",
    "                    input_text_single = input_text[batch_cnt * bs:]\n",
    "                    pre_text_tensor_valid_single = pre_text_tensor_valid[batch_cnt * bs:]\n",
    "                else:\n",
    "                    inputs_2d_single = inputs_2d[batch_cnt * bs:(batch_cnt+1) * bs]\n",
    "                    inputs_2d_flip_single = inputs_2d_flip[batch_cnt * bs:(batch_cnt+1) * bs]\n",
    "                    inputs_3d_single = inputs_3d[batch_cnt * bs:(batch_cnt+1) * bs]\n",
    "                    inputs_traj_single = inputs_traj[batch_cnt * bs:(batch_cnt + 1) * bs]\n",
    "                    input_text_single = input_text[batch_cnt * bs:(batch_cnt + 1) * bs]\n",
    "                    pre_text_tensor_valid_single = pre_text_tensor_valid[batch_cnt * bs:(batch_cnt + 1) * bs]\n",
    "                    \n",
    "                #infer_macs = profile_macs(model_eval, (inputs_2d_single, inputs_3d_single, input_text_single, pre_text_tensor_valid_single, inputs_2d_flip_single))\n",
    "                #print('INFO: Infer MACS: ', infer_macs/1000000000000, 'G')\n",
    "\n",
    "                # Measure the FPS\n",
    "                predicted_3d_pos_single = model_eval(inputs_2d_single, inputs_3d_single, input_text_single, pre_text_tensor_valid_single, input_2d_flip=inputs_2d_flip_single)[0]\n",
    "                \n",
    "                # print(predicted_3d_pos_single.shape)\n",
    "                # predicted_3d_pos_single[:, :, :, :, 0] = 0\n",
    "\n",
    "                if return_predictions:\n",
    "                    return predicted_3d_pos_single.squeeze().cpu().numpy()\n",
    "\n",
    "def get_3D_keypoints(sequence, action):\n",
    "    input_keypoints = sequence\n",
    "    ground_truth = None\n",
    "    \n",
    "    gen = UnchunkedGenerator_Seq([1], [ground_truth], [input_keypoints], action,\n",
    "                             pad=pad, causal_shift=0, augment=False,\n",
    "                             kps_left=kps_left, kps_right=kps_right, joints_left=joints_left, joints_right=joints_right)\n",
    "    prediction = evaluate(gen, return_predictions=True)\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a489bbd2-c4f7-49f9-8b7b-e89859d005c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_autoscale_on(False)\n",
    "\n",
    "# Enable subtle 3D gridlines\n",
    "ax.grid(True)\n",
    "\n",
    "def connect_joints(position_3d_per_frame, ax):\n",
    "    def plot_connection(start, end, color):\n",
    "        connected_joints_transposed = np.transpose([position_3d_per_frame[start], position_3d_per_frame[end]])\n",
    "        ax.plot3D(connected_joints_transposed[0], connected_joints_transposed[1], connected_joints_transposed[2], color=color, linewidth=2)\n",
    "\n",
    "    # Left side (green)\n",
    "    plot_connection(0, 4, 'green')  # hip - left hip\n",
    "    plot_connection(4, 5, 'green')  # left hip - left knee\n",
    "    plot_connection(5, 6, 'green')  # left knee - left foot\n",
    "    plot_connection(11, 12, 'green')  # left shoulder - left elbow\n",
    "    plot_connection(12, 13, 'green')  # left elbow - left wrist\n",
    "    plot_connection(11, 8, 'green')  # left shoulder - thorax\n",
    "\n",
    "    # Right side (blue)\n",
    "    plot_connection(0, 1, 'blue')  # hip - right hip\n",
    "    plot_connection(1, 2, 'blue')  # right hip - right knee\n",
    "    plot_connection(2, 3, 'blue')  # right knee - right foot\n",
    "    plot_connection(14, 15, 'blue')  # right shoulder - right elbow\n",
    "    plot_connection(15, 16, 'blue')  # right elbow - right wrist\n",
    "    plot_connection(14, 8, 'blue')  # right shoulder - thorax\n",
    "\n",
    "    # Middle (black)\n",
    "    plot_connection(8, 7, 'black')  # thorax - spine\n",
    "    plot_connection(8, 9, 'black')  # thorax - neck/nose\n",
    "    plot_connection(9, 10, 'black')  # neck/nose - head\n",
    "    plot_connection(7, 0, 'black')  # spine - hip\n",
    "\n",
    "def animate(i):\n",
    "    # position_3d_per_frame = np.delete(vid[i], [4,5,9,10,11,16,20,21,22,23,24,28,29,30,31], 0)\n",
    "    position_3d_per_frame = vid[i]\n",
    "\n",
    "    ax.clear()\n",
    "    ax.view_init(elev=-60, azim=-90)\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_zlim(-1, 1)\n",
    "\n",
    "    # Set empty labels for each axis instead of removing the ticks\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_zticklabels([])\n",
    "\n",
    "    # Redraw gridlines subtly\n",
    "    ax.grid(True)\n",
    "    # Set the gridline properties for a very subtle effect\n",
    "    ax.xaxis._axinfo['grid'].update(color='#D0D0D0', linestyle='-', linewidth=0.1)\n",
    "    ax.yaxis._axinfo['grid'].update(color='#D0D0D0', linestyle='-', linewidth=0.1)\n",
    "    ax.zaxis._axinfo['grid'].update(color='#D0D0D0', linestyle='-', linewidth=0.1)\n",
    "\n",
    "    connect_joints(position_3d_per_frame, ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102f4b72-c1ed-4fe1-9837-26957f5a11fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yen/.local/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "action = \"random1\"\n",
    "\n",
    "count = 0\n",
    "start_time = time()\n",
    "vid_path_name = '/Inference/'+ action + '.mp4'\n",
    "vid_name = os.path.basename(vid_path_name)[:-4]\n",
    "cap = cv2.VideoCapture(vid_path_name)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file {vid_path_name}\")\n",
    "\n",
    "vid_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "sequence = []\n",
    "vid = []\n",
    "while True:\n",
    "    ret, im = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    height, width = im.shape[:2]\n",
    "    outputs = predictor(im)\n",
    "    predicted = outputs[\"instances\"].pred_keypoints.cpu().numpy()\n",
    "\n",
    "    if len(predicted) > 0:\n",
    "        predicted = coco2h36m(predicted)[0][..., :2]\n",
    "        predicted = normalize_screen_coordinates(predicted, w=width, h=height)\n",
    "    else:\n",
    "        predicted = sequence[-1]\n",
    "    sequence.append(predicted)\n",
    "    count += 1\n",
    "\n",
    "    if count %243 == 0:\n",
    "        detected_2d = torch.tensor(np.array(sequence[-243:]))\n",
    "        predicted_3D = get_3D_keypoints(detected_2d, action)\n",
    "        # print(predicted_3D.shape)\n",
    "        vid.append(predicted_3D)\n",
    "cap.release()\n",
    "\n",
    "\n",
    "vid = np.array(vid).reshape(-1, 17, 3)\n",
    "\n",
    "detected_2d = torch.tensor(np.array(sequence[-243:]))\n",
    "predicted_3D = get_3D_keypoints(detected_2d, action)\n",
    "\n",
    "gap = count %243\n",
    "\n",
    "vid = np.concatenate([vid, predicted_3D[-gap:]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6cf482-9c7c-4466-af41-6c4111d06313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 17, 3)\n"
     ]
    }
   ],
   "source": [
    "print(vid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d08dc77-1b4a-432b-9c13-a8fd3c52bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# animate(0)\n",
    "vid[:,0,:] = 0\n",
    "ani = FuncAnimation(fig, animate, interval=10, frames = vid.shape[0])\n",
    "\n",
    "# show the plot\n",
    "# from IPython.display import HTML\n",
    "# grap2eq_out = HTML(ani.to_html5_video())\n",
    "\n",
    "ani.save('/Inference/' + action + '_3D.mp4', writer='ffmpeg', fps=vid_fps)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "FPS = end_time - start_time\n",
    "FPS = vid.shape[0] / FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b0c21d1-2654-4dc2-921b-20f1e7f7430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.682938733390367\n"
     ]
    }
   ],
   "source": [
    "print(FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9be9bfd6-ef8e-41d4-be1c-e87e3854474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped videos combined successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "video1_path = '//Inference/' + action + '.mp4'\n",
    "video2_path = '/Inference/' + action + '_3D.mp4'\n",
    "\n",
    "cap1 = cv2.VideoCapture(video1_path)\n",
    "cap2 = cv2.VideoCapture(video2_path)\n",
    "\n",
    "if not cap1.isOpened() or not cap2.isOpened():\n",
    "    raise ValueError(\"Could not open one or both videos. Check paths.\")\n",
    "\n",
    "fps = min(cap1.get(cv2.CAP_PROP_FPS), cap2.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Dimensions of the first video\n",
    "frame_width1 = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height1 = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Dimensions of the second video\n",
    "frame_width2 = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height2 = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Calculate the clipping coordinates for the middle third\n",
    "clip_start_x = frame_width2 // 3 - 30\n",
    "clip_end_x = clip_start_x * 2 + 100\n",
    "clip_width2 = clip_end_x - clip_start_x\n",
    "\n",
    "# Final frame height after resizing (match both videos' heights)\n",
    "final_height = max(frame_height1, frame_height2)\n",
    "\n",
    "# Resize widths while preserving aspect ratios\n",
    "resize_width1 = int(frame_width1 * (final_height / frame_height1))\n",
    "resize_width2 = int(clip_width2 * (final_height / frame_height2))\n",
    "final_width = resize_width1 + resize_width2\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('/Inference/' + action + '_combined_output.mp4', fourcc, fps, (final_width, final_height))\n",
    "\n",
    "while True:\n",
    "    ret1, frame1 = cap1.read()\n",
    "    ret2, frame2 = cap2.read()\n",
    "\n",
    "    if not ret1 or not ret2:\n",
    "        break\n",
    "\n",
    "    # Clip the middle third horizontally from second video\n",
    "    frame2_clipped = frame2[:, clip_start_x:clip_end_x]\n",
    "\n",
    "    # Resize frames to match heights\n",
    "    frame1_resized = cv2.resize(frame1, (resize_width1, final_height))\n",
    "    frame2_resized = cv2.resize(frame2_clipped, (resize_width2, final_height))\n",
    "\n",
    "    # Combine frames side-by-side\n",
    "    combined_frame = np.hstack((frame1_resized, frame2_resized))\n",
    "\n",
    "    out.write(combined_frame)\n",
    "\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Clipped videos combined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881640f6-3226-4ee5-a6ec-82863cfdadb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
